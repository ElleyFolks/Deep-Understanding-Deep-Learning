{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A Deeper Dive into Data Partitioning!\n",
    "- This is required for cross validation of a model's accuracy.\n",
    "- Helps to mitigate researcher overfitting by now including a test (holdout) set to truly evaluate model's accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import random_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating Fake dataset for practice.\n",
    "#### In this Project:\n",
    "- Techniques to partition data into 3 sets (train, dev, test (aka holdout)) will be covered.\n",
    "    1. Manually dividing up data.\n",
    "    2. sklearn and built in functionality to divide up data.\n",
    "\n",
    "- The dataset generated is way too small to do this pratically, it is for demonstration purposes only."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 11  12  13  14]\n",
      " [ 21  22  23  24]\n",
      " [ 31  32  33  34]\n",
      " [ 41  42  43  44]\n",
      " [ 51  52  53  54]\n",
      " [ 61  62  63  64]\n",
      " [ 71  72  73  74]\n",
      " [ 81  82  83  84]\n",
      " [ 91  92  93  94]\n",
      " [101 102 103 104]] \n",
      " [False False False False False  True  True  True  True  True]\n"
     ]
    }
   ],
   "source": [
    "# creating fake data\n",
    "fake_data = np.tile(np.array([1,2,3,4]),(10,1)) + np.tile(10*np.arange(1,11),(4,1)).T\n",
    "fake_labels = np.arange(10)>4 # divides labels between true and false\n",
    "\n",
    "# printing data and labels\n",
    "print(fake_data, \"\\n\", fake_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Partitioning Data Using `train_test_split`\n",
    "- This function only splits the dataset into 2 sets - training and testing.\n",
    "- There will be extra steps involved to divide testing into 2 sets - testing and devset (aka developer set)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fake dataset\n",
      " [[ 11  12  13  14]\n",
      " [ 21  22  23  24]\n",
      " [ 31  32  33  34]\n",
      " [ 41  42  43  44]\n",
      " [ 51  52  53  54]\n",
      " [ 61  62  63  64]\n",
      " [ 71  72  73  74]\n",
      " [ 81  82  83  84]\n",
      " [ 91  92  93  94]\n",
      " [101 102 103 104]] [False False False False False  True  True  True  True  True] \n",
      "\n",
      "Train set\n",
      " [[21 22 23 24]\n",
      " [31 32 33 34]\n",
      " [41 42 43 44]\n",
      " [81 82 83 84]\n",
      " [91 92 93 94]\n",
      " [61 62 63 64]\n",
      " [71 72 73 74]\n",
      " [11 12 13 14]] [False False False  True  True  True  True False] \n",
      "\n",
      "Devset [[51 52 53 54]] [False] \n",
      "\n",
      "Test set [[101 102 103 104]] [ True] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# setting up partitions ratios\n",
    "train_ratio = 0.8\n",
    "dev_ratio = 0.1\n",
    "test_ratio = 0.1\n",
    "\n",
    "partitions_array = [train_ratio, dev_ratio, test_ratio]\n",
    "\n",
    "# splitting data\n",
    "train_data, temp_split_data, train_labels, temp_split_labels = train_test_split(fake_data, fake_labels, train_size=partitions_array[0])\n",
    "\n",
    "# evenly splitting the temp set into dev and test sets\n",
    "devset_data, test_data, devset_labels, test_labels = train_test_split(temp_split_data, temp_split_labels, train_size=0.5)\n",
    "\n",
    "# more complicated way to determine split (only works if value for train is in first index)\n",
    "'''\n",
    "split = partitions_array[1] / np.sum(partitions_array[1:])\n",
    "devset_data, test_data, devset_labels, test_labels = train_test_split(temp_split_data, temp_split_labels, train_size= split)\n",
    "'''\n",
    "\n",
    "# printing resulting sets\n",
    "print(\"Fake dataset\\n\", fake_data, fake_labels, \"\\n\")\n",
    "print(\"Train set\\n\", train_data, train_labels, \"\\n\")\n",
    "print(\"Devset\", devset_data, devset_labels, \"\\n\")\n",
    "print(\"Test set\", test_data, test_labels, \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Partitioning Data with `random_split`\n",
    "- This function is from pytorch.utils.data.\n",
    "- The dataset must be converted to Tensors for things to work correctly.\n",
    "- I really like this way."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Split datasets\n",
      "\n",
      "Dataset 1\n",
      "\n",
      "(tensor([91., 92., 93., 94.]), tensor(1.))\n",
      "(tensor([51., 52., 53., 54.]), tensor(0.))\n",
      "(tensor([41., 42., 43., 44.]), tensor(0.))\n",
      "(tensor([61., 62., 63., 64.]), tensor(1.))\n",
      "(tensor([21., 22., 23., 24.]), tensor(0.))\n",
      "(tensor([81., 82., 83., 84.]), tensor(1.))\n",
      "(tensor([101., 102., 103., 104.]), tensor(1.))\n",
      "(tensor([31., 32., 33., 34.]), tensor(0.))\n",
      "\n",
      "\n",
      "Dataset 2\n",
      "\n",
      "(tensor([11., 12., 13., 14.]), tensor(0.))\n",
      "\n",
      "\n",
      "Dataset 3\n",
      "\n",
      "(tensor([71., 72., 73., 74.]), tensor(1.))\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# converting data to tensors\n",
    "fake_data_T = torch.Tensor(fake_data)\n",
    "fake_labels_T = torch.Tensor(fake_labels)\n",
    "\n",
    "# creating torch dataset (want to maintain relationship between data and labels)\n",
    "fake_dataset = TensorDataset(fake_data_T, fake_labels_T)\n",
    "\n",
    "# creating data loader\n",
    "fake_dataset_dl = DataLoader(fake_dataset, batch_size=2, shuffle=True)\n",
    "\n",
    "# splitting data using random_split\n",
    "split_datasets = random_split(fake_dataset, lengths= [0.8,0.1,0.1])\n",
    "\n",
    "print(\"Split datasets\\n\")\n",
    "for i, dataset in enumerate(split_datasets):\n",
    "    print(f\"Dataset {i+1}\\n\")\n",
    "    for data in dataset:\n",
    "        print(data)\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Partitioning Data Using numpy (manual)\n",
    "- Less straightforward, but you don't have to convert data to tensors.\n",
    "- In this method, 'boundaries' will be determined from the desired ratio / proportion of the data sets.\n",
    "- A list of indices from the main dataset is created, with the indices being in a randomized order.\n",
    "    - From this list the first **n** data points will go into the t*raining* set, the next **m** will go into the *dev set*, and the last **x** will go into the *test* set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Partitions: [0.8 0.1 0.1]\n",
      "Partition boundaries: [ 8  9 10]\n",
      "Random indices: [9 5 8 4 1 7 3 0 2 6]\n",
      "\n",
      "Training set: size = (8, 4)\n",
      " [[101 102 103 104]\n",
      " [ 61  62  63  64]\n",
      " [ 91  92  93  94]\n",
      " [ 51  52  53  54]\n",
      " [ 21  22  23  24]\n",
      " [ 81  82  83  84]\n",
      " [ 41  42  43  44]\n",
      " [ 11  12  13  14]] [ True  True  True False False  True False False]\n",
      "\n",
      "Devset: size = (1, 4)\n",
      " [[31 32 33 34]] [False]\n",
      "\n",
      "Test set: size = (1, 4)\n",
      " [[71 72 73 74]] [ True]\n"
     ]
    }
   ],
   "source": [
    "# setting partition ratio / proportion\n",
    "train_ratio = 0.8\n",
    "dev_ratio = 0.1\n",
    "test_ratio = 0.1\n",
    "\n",
    "partitions_array = np.array([train_ratio, dev_ratio, test_ratio])\n",
    "print(\"Partitions:\", partitions_array)\n",
    "\n",
    "# converting ratios to integers\n",
    "partition_boundaries = np.cumsum(partitions_array*len(fake_labels)).astype(int)\n",
    "print(\"Partition boundaries:\", partition_boundaries)\n",
    "\n",
    "# selecting random indices.\n",
    "random_indices = np.random.permutation(range(len(fake_labels)))\n",
    "print(\"Random indices:\", random_indices)\n",
    "\n",
    "# splitting data\n",
    "train_data_N = fake_data[random_indices[:partition_boundaries[0]],:]\n",
    "train_labels_N = fake_labels[random_indices[:partition_boundaries[0]]]\n",
    "\n",
    "devset_data_N = fake_data[random_indices[partition_boundaries[0]:partition_boundaries[1]],:]\n",
    "devset_labels_N = fake_labels[random_indices[partition_boundaries[0]:partition_boundaries[1]]]\n",
    "\n",
    "test_data_N = fake_data[random_indices[partition_boundaries[1]:],:]\n",
    "test_labels_N = fake_labels[random_indices[partition_boundaries[1]:]]\n",
    "\n",
    "# printing resulting sets\n",
    "print(f\"\\nTraining set: size = {train_data_N.shape}\\n\", train_data_N, train_labels_N)\n",
    "print(f\"\\nDevset: size = {devset_data_N.shape}\\n\", devset_data_N, devset_labels_N)\n",
    "print(f\"\\nTest set: size = {test_data_N.shape}\\n\", test_data_N, test_labels_N)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
