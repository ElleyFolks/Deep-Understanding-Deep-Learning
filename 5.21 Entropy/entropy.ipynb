{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Entropy!\n",
    "## Expanded formulas: \n",
    "1. $H = - \\displaystyle \\sum^n_{i=1}log_2(p(x_i))$\n",
    "    - units of \"bits\" and base of 2.\n",
    "<br><br>\n",
    "2. $H = - \\displaystyle \\sum^n_{i=1}log_e(p(x_i))$ or $H = - \\Sigma^n_{i=1}ln(p(x_i))$\n",
    "    - units of \"nats\" and base of e.\n",
    "<br><br>\n",
    "- in both of these equations, **x = data values**, and **p = probability**\n",
    "    - *H* represents entropy.<br><br>\n",
    "- always mind the negative sign, it is a common convention that helps the results make sense<br><br>\n",
    "\n",
    "\n",
    "## Binary Entropy\n",
    "$H = -(p*log(p) + (q)*log(q))$, where $q = 1-p$\n",
    "- known as \"binary entropy\"\n",
    "    - useful when n=2 of the first equation, and you are working with one probability.\n",
    "- Do not forget to include q, the probability of the even NOT happening.\n",
    "<br><br>\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "***\n",
    "# Using NumPy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entropy result: 0.5623351446188083\n",
      "Second method result: 0.5623351446188083\n"
     ]
    }
   ],
   "source": [
    "# exercise 1\n",
    "\n",
    "# probability of event occurring \n",
    "p = 0.25\n",
    "\n",
    "# probability of event not occurring\n",
    "q = 1-p\n",
    "\n",
    "# calculation of entropy with a for loop\n",
    "x = [0.25,0.75] # set of probabilities\n",
    "H = 0\n",
    "for p in x:\n",
    "    H -= p*np.log(p) # minus sign from formula\n",
    "\n",
    "print('Entropy result:',H)\n",
    "\n",
    "# binary entropy: simplified way to calculate entropy for 2 events\n",
    "H2 = -(p*np.log(p) + (1-p)*np.log(1-p))\n",
    "print(\"Second method result:\",H2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cross Entropy\n",
    "$H(p,q) = - \\displaystyle \\sum p*log(q)$\n",
    "- relationship between p and q, two different probability distributions\n",
    "\t- p will be probability \n",
    "\t- q will be the output from the SoftMax function\n",
    "- used to characterize performance of model\n",
    "- all probabilities p and q must sum up to 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross entropy result: 1.3862943611198906\n",
      "H of simplified equation: 1.3862943611198906\n",
      "H of further simplified equation: 1.3862943611198906\n"
     ]
    }
   ],
   "source": [
    "p = [1,0]\n",
    "q = [0.25, 0.75]\n",
    "\n",
    "# calculating cross entropy with for loop\n",
    "H3 = 0\n",
    "for i in range(len(p)): # must get length of list\n",
    "    H3 -= p[i]*np.log(q[i])\n",
    "\n",
    "print(\"Cross entropy result:\",H3)\n",
    "\n",
    "# simplified cross entropy equation when only working with 2 events\n",
    "H4 = -(p[0]*np.log(q[0] + p[1]*np.log(q[1])))\n",
    "print(\"H of simplified equation:\", H4)\n",
    "\n",
    "# further simplified for special case of p = [1,0]\n",
    "H5 = -np.log(q[0])\n",
    "print(\"H of further simplified equation:\", H5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "***\n",
    "# Using PyTorch\n",
    "- only major difference is that input must be a torch Tensor. No surprises here.\n",
    "- syntax for binary cross entropy equation:\n",
    "    1. first parameter is q, the predicted probabilities for each event\n",
    "    2. second parameter is p, \"category labels\", the event did or did not happen\n",
    "    - note: this equation is for the specific case where p = [1,0], or is binary. The event either did or did not happen, with no in between."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1.3863)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# converting q and p from previous cell to tensor data type\n",
    "p_tens = torch.Tensor(p) # note Tensor with a capitol T\n",
    "q_tens = torch.Tensor(q)\n",
    "\n",
    "# using the built in binary cross entropy function\n",
    "F.binary_cross_entropy(q_tens, p_tens)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
